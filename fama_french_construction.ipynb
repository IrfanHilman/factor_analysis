{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e0f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48b38ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/irfanhilman/factor_analysis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932dd22",
   "metadata": {},
   "source": [
    "CEK HASIL ANEH!!!!! SESUAIKAN DENGAN ANALISA, JIKA PORTFOLIO MOMENTUM HARUSNYA HASIL REGRESI RISK EXPOSURE THD MOMENTUM SIGNIFIKAN !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712f9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_1 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 1.csv')\n",
    "database_2 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 2.csv')\n",
    "database_3 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 3.csv')\n",
    "database_4 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 4.csv')\n",
    "database_5 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 5.csv')\n",
    "database_6 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 6.csv')\n",
    "database_7 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 7.csv')\n",
    "database_8 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 8.csv')\n",
    "database_9 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 9.csv')\n",
    "database_10 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 10.csv')\n",
    "database_11 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 11.csv')\n",
    "database_12 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 12.csv')\n",
    "database_13 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 13.csv')\n",
    "database_14 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 14.csv')\n",
    "database_15 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 15.csv')\n",
    "database_16 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 16.csv')\n",
    "database_17 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 17.csv')\n",
    "database_18 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 18.csv')\n",
    "database_19 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 19.csv')\n",
    "database_20 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 20.csv')\n",
    "database_21 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 21.csv')\n",
    "database_22 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 22.csv')\n",
    "database_23 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 23.csv')\n",
    "database_24 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 24.csv')\n",
    "database_25 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 25.csv')\n",
    "database_26 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 26.csv')\n",
    "database_27 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 27.csv')\n",
    "database_28 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 28.csv')\n",
    "database_29 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 29.csv')\n",
    "database_30 = pd.read_csv('/Users/irfanhilman/quanti-trading-plan/data/database_daily_update/Database Part 30.csv')\n",
    "database = pd.concat([database_1,database_2,database_3,database_4,database_5,database_6,database_7,database_8,database_9,database_10,\n",
    "                      database_11,database_12,database_13,database_14,database_15,database_16,database_17,database_18,database_19,database_20,\n",
    "                      database_21,database_22,database_23,database_24,database_25,database_26,database_27,database_28,database_29,database_30])\n",
    "database = database.drop(columns='Unnamed: 0')\n",
    "#Substitusi nilai Open, High, Low Price yg 0 dengan Close Price\n",
    "database['Open Price'] = np.where(database['Open Price'].eq(0),database['Close Price'],database['Open Price'])\n",
    "database['High Price'] = np.where(database['High Price'].eq(0),database['Close Price'],database['High Price'])\n",
    "database['Low Price'] = np.where(database['Low Price'].eq(0),database['Close Price'],database['Low Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f94a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC PARAMETER\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2025-03-21' #'2025-03-21'\n",
    "hold_period = 21 #in days\n",
    "factor = ['Book/Market','Momentum 252 Days Skip 0 Days' ] #other than Size (Market Cap)\n",
    "weighting = 'Equal-Weight' #'Value-Weight','Equal-Weight'\n",
    "hold_type = 'First-Day in Month' #'Periodic' or 'First-Day in Month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616696d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Kode', 'Date', 'Open Price', 'High Price', 'Low Price', 'Close Price',\n",
       "       'Volume', 'Transaction Value', 'foreignsell', 'foreignbuy',\n",
       "       'Quarter_Year', 'Quarter', 'Year', 'Total Pendapatan', 'Laba Kotor',\n",
       "       'Laba Usaha', 'Laba Sebelum Pajak', 'Laba Bersih Tahun Berjalan',\n",
       "       'Last Outstanding Shares', 'Total Aset Lancar',\n",
       "       'Total Aset Tidak Lancar', 'Total Aset',\n",
       "       'Total Liabilitas Jangka Pendek', 'Total Liabilitas Jangka Panjang',\n",
       "       'Total Liabilitas', 'Total Ekuitas', 'Saham Beredar',\n",
       "       'Total Arus Kas Dari Aktivitas Operasi',\n",
       "       'Total Arus Kas Dari Aktivitas Investasi',\n",
       "       'Total Arus Kas Dari Aktivitas Pendanaan', 'First Date', 'Age', 'IHSG',\n",
       "       'Market Cap', 'Median Transaction Value 3 Month',\n",
       "       'Momentum 252 Days Skip 0 Days', 'Sales/Market', 'Price/Sales',\n",
       "       'Earning/Market', 'PE Ratio', 'Book/Market', 'Previous Close Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4805914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCREENING PARAMETER\n",
    "screening_param = ['Market Cap','Total Ekuitas','Close Price','Median Transaction Value 3 Month'] #Params\n",
    "screening_sign = ['Above','Above','Above','Above'] #Above, Below, Not Equal, or Between. If Between, assign a list in screening_val: screening_val = [[min,max]]\n",
    "screening_val = [100_000_000_000,0,50,100_000_000] #Numeric variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11c99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Stocks\n",
    "drop_list = ['excl', 'ihsg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6127ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stocks(data,drop_list):\n",
    "    for r in drop_list:\n",
    "        data= data[data['Kode']!=r]\n",
    "    else: pass\n",
    "    return data\n",
    "\n",
    "def screening_stocks(data,screening_params,screening_signs,screening_vals):\n",
    "    if len(screening_params) == len(screening_signs) == len(screening_vals):\n",
    "        for ens,j in enumerate(screening_params):  #Initial Screening\n",
    "            if screening_signs[ens] == 'Above':\n",
    "                data = data[data[j]>screening_vals[ens]]\n",
    "            elif screening_signs[ens] == 'Below':\n",
    "                data = data[data[j]<screening_vals[ens]]\n",
    "            elif screening_signs[ens] == 'Not Equal':\n",
    "                data = data[data[j]!=screening_vals[ens]]\n",
    "            elif screening_signs[ens] == 'Between':\n",
    "                data = data[(data[j]>=screening_vals[ens][0])&(data[j]<=screening_vals[ens][1])]\n",
    "            else:\n",
    "                raise TypeError('screening_sign wrong. Select either Above or Below')\n",
    "    else:\n",
    "        raise Exception('Number of elements in screening parameter list are different')\n",
    "    return data\n",
    "\n",
    "def apply_weighting(data,weighting):\n",
    "        data = data.dropna(subset='Market Cap')\n",
    "        if weighting=='Equal-Weight':\n",
    "            data['Weight'] = 1/len(data)\n",
    "        elif weighting=='Value-Weight':\n",
    "            data['Weight'] = data['Market Cap']/data['Market Cap'].sum()\n",
    "        else:\n",
    "            raise TypeError('weighting wrong. See notes in the parameter setting section')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18bb71",
   "metadata": {},
   "source": [
    "Check buy_date_list dan sell_date_list beda length, harusnya sell_date_list bisa tambah 1 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac237a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_date_list = list(database['Date'].sort_values().unique())          \n",
    "if hold_type =='Periodic': \n",
    "    selected_date_list = database_date_list[::hold_period+1]\n",
    "elif hold_type =='First-Day in Month':\n",
    "    selected_date_list = []\n",
    "    database['Year'] = database['Date'].str[:4].astype('int')\n",
    "    database['Month'] = database['Date'].str[5:7].astype('int')\n",
    "    for i in database['Year'].unique():\n",
    "        database_i = database[database['Year']==i]\n",
    "        for j in database_i['Month'].unique():\n",
    "            database_j = database_i[database_i['Month']==j]\n",
    "            database_j = database_j.sort_values(by='Date')\n",
    "            date_list_j = list(database_j['Date'].unique())\n",
    "            first_day = date_list_j[0]\n",
    "            selected_date_list.append(first_day)\n",
    "    database = database.drop(columns=['Year','Month'])\n",
    "else:\n",
    "    raise TypeError('hold_type wrong. Select \"Periodic\" or \"First-Day in Month\"')\n",
    "selected_date_list = [datetime.strptime(d, \"%Y-%m-%d\") for d in selected_date_list]\n",
    "start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "buy_date_list = [d for d in selected_date_list if start_date <= d <= end_date]\n",
    "buy_date_list = [d.strftime(\"%Y-%m-%d\") for d in buy_date_list]\n",
    "sell_date_list = []\n",
    "for i in buy_date_list[1:]:\n",
    "    sell_date_i = database_date_list.index(i)-1\n",
    "    sell_date = database_date_list[sell_date_i]\n",
    "    sell_date_list.append(sell_date)\n",
    "param_date_index = [(database_date_list.index(p)-1) for p in buy_date_list]\n",
    "param_date_list = [database_date_list[p] for p in param_date_index]\n",
    "size_param = ['Date','Kode','Market Cap']\n",
    "database_part = database.copy()\n",
    "database_part = database_part[database_part['Date'].isin(param_date_list)]\n",
    "database_part = database_part.dropna(subset=['Date','Kode','Market Cap'])\n",
    "database_part = database_part.dropna(subset=factor)\n",
    "database_part = drop_stocks(data=database_part,drop_list=drop_list)\n",
    "database_part = screening_stocks(data=database_part,screening_params=screening_param,screening_signs=screening_sign,screening_vals=screening_val)\n",
    "database_part_size = database_part[size_param]\n",
    "df_small_list = []\n",
    "df_big_list = []\n",
    "for i in list(database_part_size['Date'].sort_values().unique()):\n",
    "    database_part_size_i = database_part_size[database_part_size['Date']==i]\n",
    "    database_part_size_i = database_part_size_i.sort_values(by='Market Cap')\n",
    "    df_small_i = database_part_size_i[database_part_size_i['Market Cap']<database_part_size_i['Market Cap'].median()]\n",
    "    df_big_i = database_part_size_i[database_part_size_i['Market Cap']>=database_part_size_i['Market Cap'].median()]\n",
    "    df_small_list.append(df_small_i)\n",
    "    df_big_list.append(df_big_i)\n",
    "df_small = pd.concat(df_small_list)\n",
    "df_big = pd.concat(df_big_list)\n",
    "df_factor_top_dic = {}\n",
    "df_factor_medium_dic = {}\n",
    "df_factor_bottom_dic = {}\n",
    "for f in factor:\n",
    "    df_factor_top_dic[f\"Top {f}\"] = []\n",
    "    df_factor_medium_dic[f\"Medium {f}\"] = []\n",
    "    df_factor_bottom_dic[f\"Bottom {f}\"] = []\n",
    "for f in factor:\n",
    "    for i in list(database_part['Date'].sort_values().unique()):\n",
    "        database_part_i = database_part[database_part['Date']==i]\n",
    "        database_part_factor_i = database_part_i[['Date','Kode','Market Cap',f]]\n",
    "        database_part_factor_i = database_part_factor_i.sort_values(by=f)\n",
    "        database_part_factor_i_top =database_part_factor_i[database_part_factor_i[f]>database_part_factor_i[f].quantile(0.7)]\n",
    "        database_part_factor_i_medium =database_part_factor_i[(database_part_factor_i[f]>=database_part_factor_i[f].quantile(0.3))&(database_part_factor_i[f]<=database_part_factor_i[f].quantile(0.7))]\n",
    "        database_part_factor_i_bottom =database_part_factor_i[database_part_factor_i[f]<database_part_factor_i[f].quantile(0.3)]\n",
    "        df_factor_top_dic[f\"Top {f}\"].append(database_part_factor_i_top)\n",
    "        df_factor_medium_dic[f\"Medium {f}\"].append(database_part_factor_i_medium)\n",
    "        df_factor_bottom_dic[f\"Bottom {f}\"].append(database_part_factor_i_bottom)\n",
    "for f in factor:\n",
    "    df_factor_top = pd.concat(df_factor_top_dic[f\"Top {f}\"])\n",
    "    df_factor_top = df_factor_top.reset_index(drop=True)\n",
    "    df_factor_top_dic[f\"Top {f}\"] = df_factor_top\n",
    "    df_factor_medium = pd.concat(df_factor_medium_dic[f\"Medium {f}\"])\n",
    "    df_factor_medium = df_factor_medium.reset_index(drop=True)\n",
    "    df_factor_medium_dic[f\"Medium {f}\"] = df_factor_medium\n",
    "    df_factor_bottom = pd.concat(df_factor_bottom_dic[f\"Bottom {f}\"])\n",
    "    df_factor_bottom = df_factor_bottom.reset_index(drop=True)\n",
    "    df_factor_bottom_dic[f\"Bottom {f}\"] = df_factor_bottom\n",
    "\n",
    "#Sorting portfolio and Apply weighting\n",
    "reb_comp_dic = {}\n",
    "#for f in factor:\n",
    "#    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "#        reb_comp_dic[f\"{n}{f}\"] = []\n",
    "for f in factor:\n",
    "    top_small_list = []\n",
    "    medium_small_list = []\n",
    "    bottom_small_list = []\n",
    "    top_big_list = []\n",
    "    medium_big_list = []\n",
    "    bottom_big_list = []\n",
    "    for en,i in enumerate(param_date_list[:-1]):\n",
    "        stock_top = df_factor_top_dic[f\"Top {f}\"][['Date','Kode','Market Cap']]\n",
    "        stock_top_i = stock_top[stock_top['Date']==i]\n",
    "        stock_medium = df_factor_medium_dic[f\"Medium {f}\"][['Date','Kode','Market Cap']]\n",
    "        stock_medium_i = stock_medium[stock_medium['Date']==i]\n",
    "        stock_bottom = df_factor_bottom_dic[f\"Bottom {f}\"][['Date','Kode','Market Cap']]\n",
    "        stock_bottom_i = stock_bottom[stock_bottom['Date']==i]\n",
    "        stock_small = df_small[['Date','Kode','Market Cap']]\n",
    "        stock_small_i = stock_small[stock_small['Date']==i]\n",
    "        stock_big = df_big[['Date','Kode','Market Cap']]\n",
    "        stock_big_i = stock_big[stock_big['Date']==i]\n",
    "        stock_top_small = pd.merge(stock_top_i,stock_small_i,how='inner')\n",
    "        stock_medium_small = pd.merge(stock_medium_i,stock_small_i,how='inner')\n",
    "        stock_bottom_small = pd.merge(stock_bottom_i,stock_small_i,how='inner')\n",
    "        stock_top_big = pd.merge(stock_top_i,stock_big_i,how='inner')\n",
    "        stock_medium_big = pd.merge(stock_medium_i,stock_big_i,how='inner')\n",
    "        stock_bottom_big = pd.merge(stock_bottom_i,stock_big_i,how='inner')\n",
    "        stock_top_small = apply_weighting(data=stock_top_small,weighting=weighting)\n",
    "        stock_medium_small = apply_weighting(data=stock_medium_small,weighting=weighting)\n",
    "        stock_bottom_small = apply_weighting(data=stock_bottom_small,weighting=weighting)\n",
    "        stock_top_big = apply_weighting(data=stock_top_big,weighting=weighting)\n",
    "        stock_medium_big = apply_weighting(data=stock_medium_big,weighting=weighting)\n",
    "        stock_bottom_big = apply_weighting(data=stock_bottom_big,weighting=weighting)\n",
    "        stock_top_small['From'] = buy_date_list[:-1][en]\n",
    "        stock_medium_small['From'] = buy_date_list[:-1][en]\n",
    "        stock_bottom_small['From'] = buy_date_list[:-1][en]\n",
    "        stock_top_big['From'] = buy_date_list[:-1][en]\n",
    "        stock_medium_big['From'] = buy_date_list[:-1][en]\n",
    "        stock_bottom_big['From'] = buy_date_list[:-1][en]\n",
    "        stock_top_small['To'] = sell_date_list[en]\n",
    "        stock_medium_small['To'] = sell_date_list[en]\n",
    "        stock_bottom_small['To'] = sell_date_list[en]\n",
    "        stock_top_big['To'] = sell_date_list[en]\n",
    "        stock_medium_big['To'] = sell_date_list[en]\n",
    "        stock_bottom_big['To'] = sell_date_list[en]\n",
    "        stock_top_small['Number'] = en+1\n",
    "        stock_medium_small['Number'] = en+1\n",
    "        stock_bottom_small['Number'] = en+1\n",
    "        stock_top_big['Number'] = en+1\n",
    "        stock_medium_big['Number'] = en+1\n",
    "        stock_bottom_big['Number'] = en+1\n",
    "        top_small_list.append(stock_top_small)\n",
    "        medium_small_list.append(stock_medium_small)\n",
    "        bottom_small_list.append(stock_bottom_small)\n",
    "        top_big_list.append(stock_top_big)\n",
    "        medium_big_list.append(stock_medium_big)\n",
    "        bottom_big_list.append(stock_bottom_big)\n",
    "    df_top_small = pd.concat(top_small_list)\n",
    "    df_medium_small = pd.concat(medium_small_list)\n",
    "    df_bottom_small = pd.concat(bottom_small_list)\n",
    "    df_top_big = pd.concat(top_big_list)\n",
    "    df_medium_big = pd.concat(medium_big_list)\n",
    "    df_bottom_big = pd.concat(bottom_big_list)\n",
    "    df_top_small = df_top_small[['Kode','Weight','From','To','Number']].reset_index(drop=True)\n",
    "    df_medium_small = df_medium_small[['Kode','Weight','From','To','Number']].reset_index(drop=True)\n",
    "    df_bottom_small = df_bottom_small[['Kode','Weight','From','To','Number']].reset_index(drop=True)\n",
    "    df_top_big = df_top_big[['Kode','Weight','From','To','Number']].reset_index(drop=True)\n",
    "    df_medium_big = df_medium_big[['Kode','Weight','From','To','Number']].reset_index(drop=True)\n",
    "    df_bottom_big = df_bottom_big[['Kode','Weight','From','To','Number']].reset_index(drop=True)\n",
    "    reb_comp_dic[f\"top_small_{f}\"] = df_top_small\n",
    "    reb_comp_dic[f\"medium_small_{f}\"] = df_medium_small\n",
    "    reb_comp_dic[f\"bottom_small_{f}\"] = df_bottom_small\n",
    "    reb_comp_dic[f\"top_big_{f}\"] = df_top_big\n",
    "    reb_comp_dic[f\"medium_big_{f}\"] = df_medium_big\n",
    "    reb_comp_dic[f\"bottom_big_{f}\"] = df_bottom_big\n",
    "\n",
    "\n",
    "    #for l in [top_small_list,medium_small_list,bottom_small_list,top_big_list,medium_big_list,bottom_big_list]:\n",
    "    #    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "    #        reb_comp_dic[f\"{n}{f}\"] = pd.concat(l).reset_index(drop=True)\n",
    "    #        reb_comp_dic[f\"{n}{f}\"] = reb_comp_dic[f\"{n}{f}\"][['Kode','Weight','From','To','Number']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cbf2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_sell_date_list = buy_date_list + sell_date_list\n",
    "database_price = database[['Kode','Date','Open Price','High Price','Low Price','Close Price']]\n",
    "database_price = database_price[database_price['Date'].isin(buy_sell_date_list)]\n",
    "database_price['Kode + Date'] = database_price['Kode'].astype('str') + ' ' + database_price['Date'].astype('str')\n",
    "reb_comp_act_dic = {}\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        reb_comp_act_dic[f\"{n}{f}\"] = []\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        reb_comp = reb_comp_dic[f'{n}{f}']\n",
    "        start_date = reb_comp['From'].sort_values().astype('str').values[0]\n",
    "        end_date = reb_comp['To'].sort_values().astype('str').values[-1]\n",
    "        for i in list(reb_comp['Number'].sort_values().unique()):\n",
    "            reb_comp_i = reb_comp[reb_comp['Number']==i].reset_index(drop=True)\n",
    "            stock_list_i = list(reb_comp_i['Kode'])\n",
    "            if len(stock_list_i) > len(set(stock_list_i)):\n",
    "                print('Stock list is not unique')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "            from_date = reb_comp_i['From'].astype('str').unique()[0]\n",
    "            reb_comp_act_i = pd.DataFrame({'Kode':stock_list_i})\n",
    "            reb_comp_act_i['Date'] = from_date \n",
    "            reb_comp_act_i['Kode + Date'] = reb_comp_act_i['Kode'].astype('str') + ' ' + reb_comp_act_i['Date'].astype('str')\n",
    "            reb_comp_act_i = reb_comp_act_i.merge(database_price.drop(columns=['Kode','Date']),how='left',on='Kode + Date')\n",
    "            reb_comp_act_i = reb_comp_act_i.drop(columns='Kode + Date')\n",
    "            reb_comp_act_i['Action'] = 'BUY'\n",
    "            reb_comp_act_i = reb_comp_act_i.sort_values(by='Kode').reset_index(drop=True)\n",
    "            reb_comp_act_dic[f\"{n}{f}\"].append(reb_comp_act_i)\n",
    "            to_date = reb_comp_i['To'].astype('str').unique()[0]\n",
    "            reb_comp_act_i = pd.DataFrame({'Kode':stock_list_i})\n",
    "            reb_comp_act_i['Date'] = to_date \n",
    "            reb_comp_act_i['Kode + Date'] = reb_comp_act_i['Kode'].astype('str') + ' ' + reb_comp_act_i['Date'].astype('str')\n",
    "            reb_comp_act_i = reb_comp_act_i.merge(database_price.drop(columns=['Kode','Date']),how='left',on='Kode + Date')\n",
    "            reb_comp_act_i = reb_comp_act_i.drop(columns='Kode + Date')\n",
    "            reb_comp_act_i['Action'] = 'SELL'\n",
    "            reb_comp_act_i = reb_comp_act_i.sort_values(by='Kode').reset_index(drop=True)\n",
    "            reb_comp_act_dic[f\"{n}{f}\"].append(reb_comp_act_i)\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        reb_comp_act_dic[f\"{n}{f}\"] = pd.concat(reb_comp_act_dic[f\"{n}{f}\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943a0989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book/Market\n",
      "top_small_\n",
      "medium_small_\n",
      "bottom_small_\n",
      "top_big_\n",
      "medium_big_\n",
      "bottom_big_\n",
      "Momentum 252 Days Skip 0 Days\n",
      "top_small_\n",
      "medium_small_\n",
      "bottom_small_\n",
      "top_big_\n",
      "medium_big_\n",
      "bottom_big_\n"
     ]
    }
   ],
   "source": [
    "transaction_dic = {}\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        transaction_dic[f\"{n}{f}\"] = []\n",
    "for f in factor:\n",
    "    print(f)\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        print(n)\n",
    "        reb_comp_act = reb_comp_act_dic[f\"{n}{f}\"]\n",
    "        reb_comp = reb_comp_dic[f'{n}{f}']\n",
    "        day_number = 0\n",
    "        reb_comp_act_full_day_list = []\n",
    "        for i in list(reb_comp_act['Date'].sort_values().unique()):\n",
    "            reb_comp_act_i = reb_comp_act[reb_comp_act['Date']==i]\n",
    "            reb_comp_act_i['Day Number'] = day_number\n",
    "            reb_comp_i = reb_comp[reb_comp['From']==i]\n",
    "            reb_comp_i = reb_comp_i.rename(columns={'From':'Date'})\n",
    "            reb_comp_i['Kode + Date'] = reb_comp_i['Kode'].astype('str') + ' ' + reb_comp_i['Date'].astype('str') \n",
    "            reb_comp_act_s_list = []\n",
    "            reb_comp_act_s_buy_list = []\n",
    "            for s in list(reb_comp_act_i['Kode'].sort_values()):\n",
    "                reb_comp_act_s = reb_comp_act_i[reb_comp_act_i['Kode']==s]\n",
    "                reb_comp_act_s['Initial Capital'] = 1_000_000_000\n",
    "                reb_comp_act_s['Reference Price'] = reb_comp_act_s['Close Price']\n",
    "                reb_comp_act_s['Price/Lot'] = reb_comp_act_s['Reference Price'].astype('float64')*100\n",
    "                reb_comp_act_s['Kode + Date'] = reb_comp_act_s['Kode'].astype('str') + ' ' + reb_comp_act_s['Date'].astype('str') \n",
    "                if reb_comp_act_s['Action'].values[0] == 'BUY':\n",
    "                    if day_number == 0:\n",
    "                        reb_comp_act_s['Reference Capital'] = 1_000_000_000\n",
    "                    else:\n",
    "                        reb_comp_act_s['Reference Capital'] = reb_comp_act_full_day['Total Capital'].iloc[-1]\n",
    "                    reb_comp_act_s = reb_comp_act_s.merge(reb_comp_i[['Kode + Date','Weight']],on='Kode + Date',how='left')\n",
    "                    reb_comp_act_s['Buy Price'] = reb_comp_act_s['Price/Lot']\n",
    "                    reb_comp_act_s['Sell Price'] = np.nan\n",
    "                    reb_comp_act_s['Buy Lot'] = reb_comp_act_s['Reference Capital']*reb_comp_act_s['Weight']/reb_comp_act_s['Buy Price']\n",
    "                    reb_comp_act_s['Sell Lot'] = np.nan\n",
    "                    #reb_comp_act_s['Hold Lot'] = np.nan\n",
    "                    reb_comp_act_s['Capital Allocation'] = reb_comp_act_s['Reference Capital']*reb_comp_act_s['Weight']\n",
    "                    reb_comp_act_s['Invested'] = reb_comp_act_s['Buy Price']*reb_comp_act_s['Buy Lot']\n",
    "                    reb_comp_act_s['Sold'] = np.nan\n",
    "                    reb_comp_act_s['Market Value'] = reb_comp_act_s['Close Price']*reb_comp_act_s['Buy Lot']*100\n",
    "                    #reb_comp_act_s['Potential P&L'] = reb_comp_act_s['Market Value'] - reb_comp_act_s['Invested']\n",
    "                    #reb_comp_act_s['Realized P&L'] = np.nan\n",
    "                    reb_comp_act_s['Cash'] = reb_comp_act_s['Capital Allocation'] - reb_comp_act_s['Invested']\n",
    "                    #reb_comp_act_s['ROI'] = np.nan\n",
    "                    reb_comp_act_s_buy_list.append(reb_comp_act_s)\n",
    "                elif reb_comp_act_s['Action'].values[0] == 'SELL':\n",
    "                    reb_comp_act_full_day_s = reb_comp_act_full_day[reb_comp_act_full_day['Kode']==s]\n",
    "                    reb_comp_act_s['Reference Capital'] = reb_comp_act_full_day['Total Market Value'].iloc[-1]\n",
    "                    reb_comp_act_s['Reference Price'] = reb_comp_act_s['Close Price']\n",
    "                    reb_comp_act_s['Weight'] = 0\n",
    "                    reb_comp_act_s['Buy Price'] = np.nan\n",
    "                    reb_comp_act_s['Sell Price'] = reb_comp_act_s['Price/Lot']\n",
    "                    reb_comp_act_s = reb_comp_act_s.merge(reb_comp_act_buy[['Kode','Buy Lot']],on='Kode',how='left')\n",
    "                    reb_comp_act_s = reb_comp_act_s.rename(columns={'Buy Lot':'Sell Lot'})\n",
    "                    reb_comp_act_s['Buy Lot'] = np.nan\n",
    "                    #reb_comp_act_s['Hold Lot'] = np.nan\n",
    "                    reb_comp_act_s['Capital Allocation'] = np.nan\n",
    "                    reb_comp_act_s = reb_comp_act_s.merge(reb_comp_act_full_day_s[['Kode','Invested']],on='Kode',how='left')\n",
    "                    reb_comp_act_s['Sold'] = reb_comp_act_s['Sell Price']*reb_comp_act_s['Sell Lot']\n",
    "                    reb_comp_act_s['Market Value'] = np.nan\n",
    "                    #reb_comp_act_s['Potential P&L'] = np.nan\n",
    "                    #reb_comp_act_s['Realized P&L'] = reb_comp_act_s['Sold']-reb_comp_act_s['Invested']\n",
    "                    reb_comp_act_s = reb_comp_act_s.merge(reb_comp_act_full_day_s[['Kode','Cash']],on='Kode',how='left')\n",
    "                    reb_comp_act_s['Cash'] = reb_comp_act_s['Cash'] + reb_comp_act_s['Sold']\n",
    "                    sold = reb_comp_act_s['Sold'].iloc[0]\n",
    "                    invested = reb_comp_act_buy[reb_comp_act_buy['Kode']==s]['Invested'].iloc[0]\n",
    "                    #reb_comp_act_s['ROI'] = (sold-invested)/invested\n",
    "                else: pass\n",
    "                reb_comp_act_s_list.append(reb_comp_act_s)\n",
    "            reb_comp_act_full_day = pd.concat(reb_comp_act_s_list)\n",
    "            reb_comp_act_full_day['Total Invested'] = reb_comp_act_full_day['Invested'].sum()\n",
    "            reb_comp_act_full_day['Total Market Value'] = reb_comp_act_full_day['Market Value'].sum()\n",
    "            #reb_comp_act_full_day['Total Potential P&L'] = reb_comp_act_full_day['Potential P&L'].sum()\n",
    "            #reb_comp_act_full_day['Total Realized P&L'] = reb_comp_act_full_day['Realized P&L'].sum()\n",
    "            reb_comp_act_full_day['Total Cash'] = reb_comp_act_full_day['Cash'].sum() \n",
    "            reb_comp_act_full_day['Total Capital'] = reb_comp_act_full_day['Total Market Value'] + reb_comp_act_full_day['Total Cash']\n",
    "            reb_comp_act_full_day['Weight'] = reb_comp_act_full_day['Weight'].fillna(value=reb_comp_act_full_day['Market Value']/reb_comp_act_full_day['Total Market Value'])\n",
    "            day_number = day_number + 1\n",
    "            transaction_dic[f\"{n}{f}\"].append(reb_comp_act_full_day)\n",
    "            if len(reb_comp_act_s_buy_list)>0:\n",
    "                reb_comp_act_buy = pd.concat(reb_comp_act_s_buy_list)\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        transaction_dic[f\"{n}{f}\"] = pd.concat(transaction_dic[f\"{n}{f}\"]).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc882b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_capital_dic = {}\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        hist_capital_dic[f\"{n}{f}\"] = []\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        transaction = transaction_dic[f\"{n}{f}\"]\n",
    "        transaction_buy = transaction[transaction['Action']=='BUY']\n",
    "        transaction_buy = transaction_buy.sort_values(by='Date')\n",
    "        for i in transaction_buy['Date'].unique():\n",
    "            transaction_buy_i = transaction_buy[transaction_buy['Date']==i]\n",
    "            transaction_buy_i = transaction_buy_i[['Date','Total Capital']]\n",
    "            transaction_buy_i = transaction_buy_i.head(1)\n",
    "            hist_capital_dic[f\"{n}{f}\"].append(transaction_buy_i)\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        hist_capital_i = pd.concat(hist_capital_dic[f\"{n}{f}\"])\n",
    "        hist_capital_i['Return'] = hist_capital_i['Total Capital'].pct_change()\n",
    "        hist_capital_i = hist_capital_i.reset_index(drop=True)\n",
    "        hist_capital_dic[f\"{n}{f}\"] = hist_capital_i\n",
    "ret = []\n",
    "for f in factor:\n",
    "    for n in ['top_small_','medium_small_','bottom_small_','top_big_','medium_big_','bottom_big_']:\n",
    "        hist_ret = hist_capital_dic[f\"{n}{f}\"][['Date','Return']]\n",
    "        hist_ret = hist_ret.set_index('Date',drop=True)\n",
    "        hist_ret = hist_ret.rename(columns={'Return':f'{n}{f}'})\n",
    "        ret.append(hist_ret)\n",
    "df_return = pd.concat(ret,axis=1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa503fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = []\n",
    "for en,f in enumerate(factor):\n",
    "    df_ret = df_return.copy()\n",
    "    df_ret[f'SMB_F{en+1}'] = ((df_ret[f'top_small_{f}']+df_ret[f'medium_small_{f}']+df_ret[f'bottom_small_{f}'])/3)-((df_ret[f'top_big_{f}']+df_ret[f'medium_big_{f}']+df_ret[f'bottom_big_{f}'])/3)\n",
    "    ff.append(df_ret[f'SMB_F{en+1}'])\n",
    "df_ff = pd.concat(ff,axis=1).dropna()\n",
    "n_f = len(df_ff.columns)\n",
    "df_ff['SMB'] = df_ff.sum(axis=1)/n_f\n",
    "\n",
    "#Tambahkan IHSG\n",
    "ihsg = pd.read_excel('/Users/irfanhilman/create_database/additional_data/IHSG.xls',skiprows=[0])\n",
    "ihsg = ihsg[['Tanggal','Nilai']]\n",
    "ihsg = ihsg.rename(columns={'Tanggal':'Date','Nilai':'IHSG'})\n",
    "ihsg = ihsg[ihsg['Date'].isin(buy_date_list)]\n",
    "ihsg['Date'] = ihsg['Date'].astype('str')\n",
    "ihsg = ihsg.set_index('Date',drop=True)\n",
    "ihsg = ihsg.pct_change()\n",
    "ihsg = ihsg.dropna()\n",
    "\n",
    "#Tambahkan Rf\n",
    "rf = pd.read_excel('/Users/irfanhilman/create_database/additional_data/Yield_Obligasi_Pemerintah_Indonesia_10_tahun.xls',skiprows=[0])\n",
    "rf = rf.rename(columns={'Tanggal':'Date','Nilai':'Rf'})\n",
    "rf = rf.drop(columns='Indeks')\n",
    "rf = rf[rf['Date'].isin(buy_date_list)]\n",
    "rf['Date'] = rf['Date'].astype(str)\n",
    "rf = rf.set_index('Date',drop=True)\n",
    "rf['Rf'] = (rf['Rf']*0.01)/12\n",
    "\n",
    "df_ff = df_ff.join(ihsg)\n",
    "df_ff = df_ff.join(rf)\n",
    "df_ff[['IHSG','Rf']] = df_ff[['IHSG','Rf']].ffill()\n",
    "df_ff['Mkt - Rf'] = df_ff['IHSG'] - df_ff['Rf']\n",
    "\n",
    "#Custom made\n",
    "hml = factor[0]\n",
    "mom = factor[1]\n",
    "df_ff['HML'] = ((df_return[f'top_small_{hml}'] + df_return[f'top_big_{hml}'])/2)-((df_return[f'bottom_small_{hml}'] + df_return[f'bottom_big_{hml}'])/2)\n",
    "df_ff['MOM'] = ((df_return[f'top_small_{mom}'] + df_return[f'top_big_{mom}'])/2)-((df_return[f'bottom_small_{mom}'] + df_return[f'bottom_big_{mom}'])/2)\n",
    "\n",
    "df_ff = df_ff[['Mkt - Rf','SMB','HML','MOM','Rf']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37cf009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff.to_excel(f'FF Portfolio {weighting} {hold_type}.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
